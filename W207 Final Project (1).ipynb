{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport random\n\nimport matplotlib.pyplot as plt\nfrom keras.utils import to_categorical\n\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nprint(os.listdir(\"../input/movie-review-sentiment-analysis-kernels-only\"))\nprint(os.listdir(\"../input/word2vec-google\"))\nprint(os.listdir(\"../input/fasttext-english-word-vectors-including-subwords\"))\n\nnp.random.seed(0)\nrandom.seed(0)\n\npd.set_option('display.max_colwidth', -1)\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"stream","text":"['movie-review-sentiment-analysis-kernels-only', 'word2vec-google', 'fasttext-english-word-vectors-including-subwords']\n['train.tsv', 'test.tsv', 'sampleSubmission.csv']\n['GoogleNews-vectors-negative300.bin']\n['wiki-news-300d-1M-subword.vec']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/movie-review-sentiment-analysis-kernels-only/train.tsv', sep='\\t')\n\ntrain = data[['PhraseId','SentenceId','Phrase']]\nlabels = data[['Sentiment']]\nprint(train.shape)\nprint(labels.shape)","execution_count":2,"outputs":[{"output_type":"stream","text":"(156060, 3)\n(156060, 1)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preprocess text data\nimport re\n\ndef preprocess(text):\n    words = []\n    split = re.compile(r\"[^a-z0-9']\")\n    digit = re.compile(r'^\\d')\n    alpha = re.compile(r'[a-z]')\n    \n    contractions = {\n        \"n't\": 'not',\n        \"'d\": 'would',\n        \"'re\": 'are',\n        \"'ll\": 'will',\n        \"'s\": 'has',\n        \"'m\": 'am',\n        \"'ve\": 'have'\n    }\n    \n    for w in split.split(text.lower()):\n        if w.isspace():\n            continue\n        if w == '':\n            continue\n        if w in contractions:\n            w = contractions[w]\n        if digit.match(w):\n            continue\n        if not alpha.match(w):\n            continue\n\n        words.append(w)\n            \n    return ' '.join(words)\n\ntrain['CleanedPhrase'] = train['Phrase'].apply(preprocess)\ntrain.head()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"   PhraseId                                                                                            ...                                                                                                                                                                                                                                                                        CleanedPhrase\n0  1                                                                                                   ...                                                                                             a series of escapades demonstrating the adage that what is good for the goose is also good for the gander some of which occasionally amuses but none of which amounts to much of a story\n1  2                                                                                                   ...                                                                                             a series of escapades demonstrating the adage that what is good for the goose                                                                                                           \n2  3                                                                                                   ...                                                                                             a series                                                                                                                                                                                \n3  4                                                                                                   ...                                                                                             a                                                                                                                                                                                       \n4  5                                                                                                   ...                                                                                             series                                                                                                                                                                                  \n\n[5 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PhraseId</th>\n      <th>SentenceId</th>\n      <th>Phrase</th>\n      <th>CleanedPhrase</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .</td>\n      <td>a series of escapades demonstrating the adage that what is good for the goose is also good for the gander some of which occasionally amuses but none of which amounts to much of a story</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>A series of escapades demonstrating the adage that what is good for the goose</td>\n      <td>a series of escapades demonstrating the adage that what is good for the goose</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>A series</td>\n      <td>a series</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>A</td>\n      <td>a</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>series</td>\n      <td>series</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# determine which records are full sentences\ntrain['Tokens'] = train['CleanedPhrase'].str.split(' ').str.len()\nsentences = train.groupby('SentenceId', sort=False)['Tokens'].transform(max) == train['Tokens']\ntrain['Full'] = sentences","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train.Full].head(10)","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"     PhraseId  SentenceId  ...  Tokens  Full\n0    1         1           ...   35     True\n63   64        2           ...   9      True\n81   82        3           ...   18     True\n116  117       4           ...   23     True\n117  118       4           ...   23     True\n156  157       5           ...   7      True\n157  158       5           ...   7      True\n166  167       6           ...   19     True\n198  199       7           ...   8      True\n213  214       8           ...   19     True\n\n[10 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PhraseId</th>\n      <th>SentenceId</th>\n      <th>Phrase</th>\n      <th>CleanedPhrase</th>\n      <th>Tokens</th>\n      <th>Full</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .</td>\n      <td>a series of escapades demonstrating the adage that what is good for the goose is also good for the gander some of which occasionally amuses but none of which amounts to much of a story</td>\n      <td>35</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>63</th>\n      <td>64</td>\n      <td>2</td>\n      <td>This quiet , introspective and entertaining independent is worth seeking .</td>\n      <td>this quiet introspective and entertaining independent is worth seeking</td>\n      <td>9</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>81</th>\n      <td>82</td>\n      <td>3</td>\n      <td>Even fans of Ismail Merchant 's work , I suspect , would have a hard time sitting through this one .</td>\n      <td>even fans of ismail merchant has work i suspect would have a hard time sitting through this one</td>\n      <td>18</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>116</th>\n      <td>117</td>\n      <td>4</td>\n      <td>A positively thrilling combination of ethnography and all the intrigue , betrayal , deceit and murder of a Shakespearean tragedy or a juicy soap opera .</td>\n      <td>a positively thrilling combination of ethnography and all the intrigue betrayal deceit and murder of a shakespearean tragedy or a juicy soap opera</td>\n      <td>23</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>117</th>\n      <td>118</td>\n      <td>4</td>\n      <td>A positively thrilling combination of ethnography and all the intrigue , betrayal , deceit and murder of a Shakespearean tragedy or a juicy soap opera</td>\n      <td>a positively thrilling combination of ethnography and all the intrigue betrayal deceit and murder of a shakespearean tragedy or a juicy soap opera</td>\n      <td>23</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>156</th>\n      <td>157</td>\n      <td>5</td>\n      <td>Aggressive self-glorification and a manipulative whitewash .</td>\n      <td>aggressive self glorification and a manipulative whitewash</td>\n      <td>7</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>157</th>\n      <td>158</td>\n      <td>5</td>\n      <td>Aggressive self-glorification and a manipulative whitewash</td>\n      <td>aggressive self glorification and a manipulative whitewash</td>\n      <td>7</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>166</th>\n      <td>167</td>\n      <td>6</td>\n      <td>A comedy-drama of nearly epic proportions rooted in a sincere performance by the title character undergoing midlife crisis .</td>\n      <td>a comedy drama of nearly epic proportions rooted in a sincere performance by the title character undergoing midlife crisis</td>\n      <td>19</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>198</th>\n      <td>199</td>\n      <td>7</td>\n      <td>Narratively , Trouble Every Day is a plodding mess .</td>\n      <td>narratively trouble every day is a plodding mess</td>\n      <td>8</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>213</th>\n      <td>214</td>\n      <td>8</td>\n      <td>The Importance of Being Earnest , so thick with wit it plays like a reading from Bartlett 's Familiar Quotations</td>\n      <td>the importance of being earnest so thick with wit it plays like a reading from bartlett has familiar quotations</td>\n      <td>19</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_class_weights(labels):\n    counts = np.bincount(labels)\n    largest = np.max(counts)\n\n    w = {}\n    for label, count in enumerate(counts):\n        w[label] = largest/count\n\n    print('Weights:')\n    for cls, weight in w.items():\n        print('%d: %0.6f' % (cls, weight))","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train, labels, test_size=0.20)\nX_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.50)\n\ny_train = y_train['Sentiment'].values\ny_test = y_test['Sentiment'].values\ny_val = y_val['Sentiment'].values\n\nn_classes = len(np.unique(y_train))\n\nprint('Classes: ', n_classes)\nprint('Training size: %s, %s' % (X_train.shape, y_train.shape))\nprint('Testing size: %s, %s' % (X_test.shape, y_test.shape))\nprint('Validation size: %s, %s' % (X_val.shape, y_val.shape))","execution_count":7,"outputs":[{"output_type":"stream","text":"Classes:  5\nTraining size: (124848, 6), (124848,)\nTesting size: (15606, 6), (15606,)\nValidation size: (15606, 6), (15606,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"weights = calculate_class_weights(y_train)","execution_count":8,"outputs":[{"output_type":"stream","text":"Weights:\n0: 11.422414\n1: 2.914757\n2: 1.000000\n3: 2.401269\n4: 8.624898\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_history(history, name):\n    # Plot training & validation accuracy values\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.title('Model %s accuracy' % name)\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n\n    # Plot training & validation loss values\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model %s loss' % name)\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n    \ndef train_model(name, model, x_train, y_train, x_test, y_test, x_val, y_val,\n                batch_size=32, epochs=10, callbacks=[], class_weight=weights, categorical=True, initial_epoch=0, save=True):\n    if categorical:\n        y_train = to_categorical(y_train)\n        y_test = to_categorical(y_test)\n        y_val = to_categorical(y_val)\n        \n    history = model.fit(x_train,\n          batch_size=batch_size, \n          y=y_train,\n          verbose=1,\n          shuffle=True, \n          epochs=epochs, \n          validation_data=(x_test, y_test),\n          class_weight=class_weight,\n          callbacks=callbacks,\n          initial_epoch=initial_epoch)\n\n    if save:\n        model.save('%s.h5' % name)\n        \n    plot_history(history, name)\n    scores = model.evaluate(x_val, y_val)\n\n    print('validation scores for %s' % name)\n    for i, score in enumerate(scores):\n        name = model.metrics_names[i]\n        print('%s score: %0.6f' % (name, score))\n    \n    return history","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\n# cv = CountVectorizer()\n# cv.fit(X_train['CleanedPhrase'])\n\n# tv = TfidfVectorizer()\n# tv.fit(X_train['CleanedPhrase'])\n\n# x_train = tv.transform(X_train['CleanedPhrase'])\n# x_test = tv.transform(X_test['CleanedPhrase'])\n# x_val = tv.transform(X_val['CleanedPhrase'])\n\n# print('Items in training vocabulary (CountVectorizer): ', len(cv.vocabulary_))\n# print('Items in training vocabulary (TfidfVectorizer): ', len(tv.vocabulary_))\n\n#vocab_length = len(cv.vocabulary_)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Input, Dense, Embedding, Flatten, Dropout, LSTM, GRU, RNN, Bidirectional\nfrom keras.models import Model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom keras.layers import SpatialDropout1D, BatchNormalization\nfrom keras.initializers import Constant","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seq_max_length=50\n\ndef create_sequences(tokenizer, *inputs, **kwargs):\n    maxlen = kwargs.get('maxlen', seq_max_length)\n    padding = kwargs.get('padding', 'pre')\n    fit = kwargs.get('fit', True)\n    \n    if fit:\n        tokenizer.fit_on_texts(inputs[0])\n    \n    seqs = []\n    for input in inputs:\n        seq = tokenizer.texts_to_sequences(input)\n        seq = pad_sequences(seq, maxlen=maxlen, padding=padding)\n        seqs.append(seq)\n    \n    return seqs","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(filters='', lower=True)\n(train_seq, test_seq, val_seq) = create_sequences(tokenizer,\n                                                  X_train['CleanedPhrase'],\n                                                  X_test['CleanedPhrase'], \n                                                  X_val['CleanedPhrase'],\n                                                  padding='post', fit=True)\n\nprint(train_seq.shape)\nprint(test_seq.shape)\nprint(val_seq.shape)","execution_count":13,"outputs":[{"output_type":"stream","text":"(124848, 50)\n(15606, 50)\n(15606, 50)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_full_train = X_train[X_train.Full]\n# y_full_train = y_train[X_train.Full]\n\n# X_full_test = X_test[X_test.Full]\n# y_full_test = y_test[X_test.Full]\n\n# X_full_val = X_val[X_val.Full]\n# y_full_val = y_val[X_val.Full]\n\n# print(X_full_train.shape)\n# print(X_full_test.shape)\n# print(X_full_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# full_tokenizer = Tokenizer()\n# (full_train_seq, full_test_seq, full_val_seq) = \\\n#     create_sequences(full_tokenizer,\n#                      X_full_train['CleanedPhrase'],\n#                      X_full_test['CleanedPhrase'],\n#                      X_full_val['CleanedPhrase'],\n#                      padding='post', fit=True)\n\n# print(full_train_seq.shape)\n# print(full_test_seq.shape)\n# print(full_val_seq.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# full_weights = calculate_class_weights(y_full_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def build_model_v1(vocab_size, n_classes=n_classes, loss='categorical_crossentropy', optimizer='adam', metrics=['acc']):\n#     inputs = Input(shape=(vocab_size, ), name='input')\n    \n#     x = Dense(5000, name='dense1', activation='relu')(inputs)\n#     x = Dropout(0.6)(x)\n#     x = Dense(1000, name='dense2', activation='relu')(x)\n#     x = Dropout(0.4)(x)\n#     x = Dense(100, name='dense3', activation='relu')(x)\n#     x = Dropout(0.3)(x)\n#     x = Dense(n_classes, name='dense4', activation='softmax')(x)\n    \n#     model = Model(inputs=inputs, outputs=x)\n#     model.compile(optimizer=optimizer,\n#                   loss=loss,\n#                   metrics=metrics)\n    \n#     return model\n\n# model1 = build_model_v1(len(cv.vocabulary_), n_classes=n_classes)\n# model1.summary()\n\n# train_model('model1', model1, x_train, y_train, x_test, y_test, x_val, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def build_model_v2(vocab_size, max_length, embedding_output_dims=100, n_classes=n_classes, loss='categorical_crossentropy', optimizer='adam', metrics=['acc']):\n#     inputs = Input(shape=(max_length,))\n    \n#     x = Embedding(vocab_size, embedding_output_dims, input_length=max_length)(inputs)\n#     x = Flatten()(x)\n#     x = Dense(1000, activation='relu')(x)\n#     x = Dropout(0.8)(x)\n#     x = Dense(500, activation='relu')(x)\n#     x = Dropout(0.8)(x)\n#     x = Dense(100, activation='relu')(x)\n#     x = Dropout(0.8)(x)\n#     x = Dense(n_classes, activation='softmax')(x)\n\n#     model = Model(inputs=[inputs], outputs=x)\n#     model.compile(optimizer=optimizer,\n#                   loss=loss,\n#                   metrics=metrics)\n    \n#     return model\n\n# model2 = build_model_v2(len(tokenizer.word_index), seq_max_length)\n# model2.summary()\n\n# train_model('model2', model2, train_seq, y_train, test_seq, y_test, val_seq, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def build_model_v3(vocab_size, max_length, lstm_hidden_size=100, embedding_output_dims=100, n_classes=n_classes, loss='categorical_crossentropy', optimizer='adam', metrics=['acc']):\n#     inputs = Input(shape=(max_length,))\n    \n#     x = Embedding(vocab_size, embedding_output_dims, input_length=max_length)(inputs)\n#     x = LSTM(lstm_hidden_size, dropout=0.6, recurrent_dropout=0.6, return_sequences=False)(x)\n#     x = Dense(n_classes, activation='softmax')(x)\n\n#     model = Model(inputs=[inputs], outputs=x)\n#     model.compile(optimizer=optimizer,\n#                   loss=loss,\n#                   metrics=metrics)\n    \n#     return model\n\n# model3 = build_model_v3(vocab_length, max_length)\n# model3.summary()\n\n# train_model('model3', model3, train_seq, y_train, test_seq, y_test, val_seq, y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def predict_v1(text):\n#     input = cv.transform([text])\n#     output = model.predict(input)\n#     predicted = np.argmax(output)\n#     return ['negative', 'somewhat negative', 'neutral', 'somewhat positive', 'positive'][predicted]\n    \n# predict_v1('It had a lot of humor.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim\nfrom gensim.models import Word2Vec\nfrom gensim.utils import simple_preprocess\nfrom gensim.models.wrappers import FastText\n\n# word2vec\nfrom gensim.models.keyedvectors import KeyedVectors\nword_vectors = KeyedVectors.load_word2vec_format('../input/word2vec-google/GoogleNews-vectors-negative300.bin', binary=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dims = 300\n\ndef get_embedding_matrix(tokenier, dims=embedding_dims, random_unknowns=True):\n    vocab_length = len(tokenizer.word_index) + 1\n\n    embedding_matrix = np.zeros((vocab_length, dims))\n    missing_words = 0\n    for word, i in tokenizer.word_index.items():\n        if word in word_vectors:\n            embedding_vector = word_vectors[word]\n            embedding_matrix[i] = embedding_vector\n        else:\n            missing_words += 1\n            # randomly initialize a vector for unknown words\n            if random_unknowns:\n                embedding_matrix[i] = np.random.normal(0,np.sqrt(0.25), dims)\n            #print('not in index: ', word)\n\n    print('missing %d words' % missing_words)\n\n    return (embedding_matrix, vocab_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(embedding_matrix, vocab_length) = get_embedding_matrix(tokenizer)\n#(full_embedding_matrix, full_vocab_length) = get_embedding_matrix(full_tokenizer)\n\nprint(embedding_matrix.shape)\n#print(full_embedding_matrix.shape)\nprint(vocab_length)\n#print(full_vocab_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del(word_vectors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from keras.callbacks import Callback, LearningRateScheduler\n# import math\n\n# def step_decay(initial_lr=0.001, drop=0.5, epochs_drop=10.0):\n#     return lambda epoch: initial_lr * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n\n# class LossHistory(Callback):\n#     def on_train_begin(self, logs={}):\n#        self.losses = []\n#        self.lr = []\n \n#     def on_epoch_end(self, batch, logs={}):\n#        self.losses.append(logs.get('loss'))\n#        self.lr.append(step_decay(len(self.losses)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def build_model_v4(vocab_size,\n#                    max_length,\n#                    rnn_units=100,\n#                    embedding_dims=100,\n#                    n_classes=n_classes,\n#                    loss='categorical_crossentropy',\n#                    optimizer='adam',\n#                    metrics=['acc'],\n#                    dropout=0,\n#                    recurrent_dropout=0,\n#                    embedding_weights=None,\n#                    activation='softmax',\n#                   rnn_type='lstm',\n#                   batch_normalization=False,\n#                   dense_units=512,\n#                   spatial_dropout=0):\n#     inputs = Input(shape=(max_length,))\n    \n#     trainable = True\n#     initializer='uniform'\n#     if embedding_weights is not None:\n#         trainable = False\n#         initializer=Constant(embedding_weights)\n        \n#     x = Embedding(vocab_size,\n#                   embedding_dims,\n#                   input_length=max_length,\n#                   trainable=trainable,\n#                   embeddings_initializer=initializer)(inputs)\n    \n#     if spatial_dropout > 0:\n#         x = SpatialDropout1D(spatial_dropout)(x)\n    \n#     if rnn_type == 'lstm':\n#         x = LSTM(rnn_units, return_sequences=False, dropout=dropout, recurrent_dropout=recurrent_dropout)(x)\n#     elif rnn_type == 'bilstm':\n#         x = Bidirectional(LSTM(rnn_units, return_sequences=False, dropout=dropout, recurrent_dropout=recurrent_dropout))(x)\n#     elif rnn_type == 'gru':\n#         x = GRU(rnn_units, return_sequences=False, dropout=dropout, recurrent_dropout=recurrent_dropout)(x)\n    \n#     if batch_normalization:\n#         x = Dense(dense_units)(x)\n#         x = BatchNormalization()(x)\n        \n#     x = Dense(n_classes, activation=activation)(x)\n\n#     model = Model(inputs=[inputs], outputs=x)\n#     model.compile(optimizer=optimizer,\n#                   loss=loss,\n#                   metrics=metrics)\n    \n#     return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lstm_units = 100\n# model4 = build_model_v4(vocab_length, max_length,\n#                         rnn_units=lstm_units,\n#                         embedding_dims=embedding_dims,\n#                         embedding_weights=embedding_matrix)\n# model4.summary()\n\n# # loss_history = LossHistory()\n# # lrate = LearningRateScheduler(step_decay)\n# train_model('model4', model4, \n#             train_seq, y_train, test_seq, y_test, val_seq, y_val,\n#             #full_train_seq, y_full_train, full_test_seq, y_full_test, full_val_seq, y_full_val,\n#             #callbacks=[loss_history, lrate], \n#             class_weight=weights,\n#             epochs=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model5 = build_model_v4(vocab_length, max_length,\n#                         rnn_units=lstm_units,\n#                         embedding_dims=embedding_dims,\n#                         embedding_weights=embedding_matrix,\n#                         rnn_type='gru')\n# model5.summary()\n\n# # loss_history = LossHistory()\n# # lrate = LearningRateScheduler(step_decay)\n\n# train_model('model5', model5, \n#             #train_seq, y_train, test_seq, y_test, val_seq, y_val,\n#             full_train_seq, y_full_train, full_test_seq, y_full_test, full_val_seq, y_full_val,\n#             #callbacks=[loss_history, lrate], \n#             class_weight=full_weights,\n#             epochs=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model6 = build_model_v4(full_vocab_length,\n#                         max_length,\n#                         rnn_units=64,\n#                         embedding_dims=300,\n#                         embedding_weights=full_embedding_matrix,\n#                         rnn_type='gru',\n#                         spatial_dropout=0.7,\n#                         recurrent_dropout=0.2,\n#                         dropout=0.2)\n# model6.summary()\n\n# # loss_history = LossHistory()\n# # lrate = LearningRateScheduler(step_decay)\n\n# train_model('model6', model6, \n#             full_train_seq, y_full_train, full_test_seq, y_full_test, full_val_seq, y_full_val,\n#             #callbacks=[loss_history, lrate], \n#             class_weight=full_weights,\n#             epochs=25,\n#             batch_size=25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model7 = []\n# for i in range(n_classes):\n#     model = build_model_v4(full_vocab_length,\n#                         max_length,\n#                         rnn_units=32,\n#                         embedding_dims=embedding_dims,\n#                         embedding_weights=full_embedding_matrix,\n#                         dropout=0.2,\n#                         recurrent_dropout=0.2,\n#                         rnn_type='gru',\n#                         n_classes=1,\n#                         loss='binary_crossentropy',\n#                         activation='sigmoid')\n#     model.summary()\n    \n#     y_tr = np.where(y_full_train == i, 1, 0)\n#     y_t = np.where(y_full_test == i, 1, 0)\n#     y_v = np.where(y_full_val == i, 1, 0)\n    \n#     class_weights = calculate_class_weights(y_tr)\n    \n#     loss_history = LossHistory()\n#     lrate = LearningRateScheduler(step_decay)\n    \n#     train_model('model7.%d' % i, model, \n#             full_train_seq, y_tr, full_test_seq, y_t, full_val_seq, y_v,\n#             class_weight=class_weights,\n#             categorical=False,\n#             callbacks=[loss_history,lrate],\n#             epochs=10)\n    \n#     model7.append(model)\n#     del(y_tr)\n#     del(y_t)\n#     del(y_v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from keras.layers import Conv1D, MaxPooling1D, Flatten, LeakyReLU\n# from keras import regularizers\n# from keras.callbacks import ModelCheckpoint\n\n# def build_model_v8(vocab_size, max_length,\n#                    embedding_dims=100,\n#                    n_classes=n_classes,\n#                    dropouts=[0,0,0,0],\n#                    loss='categorical_crossentropy',\n#                    optimizer='adam',\n#                    dense_units=100,\n#                    conv_filters=[32,32],\n#                    conv_kernels=[3,3],\n#                    pool_sizes=[2,2],\n#                    conv_padding='same',\n#                    conv_regularization=None,\n#                    dense_regularizations=[None,None],\n#                    embedding_weights=None,\n#                    metrics=['acc']):\n#     inputs = Input(shape=(max_length,))\n    \n#     trainable = True\n#     embedding_initializer = 'uniform'\n#     if embedding_weights is not None:\n#         trainable = False\n#         embedding_initializer = Constant(embedding_weights)\n    \n#     x = Embedding(vocab_size, embedding_dims,\n#                   input_length=max_length,\n#                   trainable=trainable,\n#                   embeddings_initializer=embedding_initializer)(inputs)\n    \n#     x = Conv1D(filters=conv_filters[0], kernel_size=conv_kernels[0], padding=conv_padding, kernel_regularizer=conv_regularization)(x)\n#     x = LeakyReLU()(x)\n#     x = MaxPooling1D(pool_size=pool_sizes[0])(x)\n#     x = SpatialDropout1D(dropouts[0])(x)\n    \n#     x = Conv1D(filters=conv_filters[1], kernel_size=conv_kernels[1], padding=conv_padding, kernel_regularizer=conv_regularization)(x)\n#     x = LeakyReLU()(x)\n#     x = MaxPooling1D(pool_size=pool_sizes[1])(x)\n#     x = SpatialDropout1D(dropouts[1])(x)\n    \n#     x = Flatten()(x)\n#     x = Dropout(dropouts[1])(x)\n#     x = Dense(dense_units, activation='sigmoid', kernel_regularizer=dense_regularizations[0])(x)\n#     x = Dropout(dropouts[2])(x)\n#     x = Dense(n_classes, activation='softmax', kernel_regularizer=dense_regularizations[1])(x)\n\n#     model = Model(inputs=[inputs], outputs=x)\n#     model.compile(optimizer=optimizer,\n#                   loss=loss,\n#                   metrics=metrics)\n    \n#     return model\n\n# model8 = build_model_v8(len(tokenizer.word_index),\n#                         seq_max_length,\n#                         embedding_dims=128,\n#                         dense_units=500,\n#                         conv_regularization=regularizers.l2(0.01),\n#                         dropouts=[0.7,0.7,0.7,0.5],\n#                         conv_filters=[64,32],\n#                         conv_kernels=[3,3])\n# model8.summary()\n\n# filepath=\"model8.h5\"\n# checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n\n# train_model('model8', model8, \n#             train_seq, y_train, test_seq, y_test, val_seq, y_val,\n#             #callbacks=[loss_history, lrate], \n#             callbacks=[checkpoint],\n#             class_weight=weights,\n#             epochs=10,\n#             batch_size=25,\n#             save=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def evaluate(text, true=None):\n#     if isinstance(text, str):\n#         df = pd.DataFrame([[text]], columns=['text'])\n#         text = df.text\n    \n#     text = text.apply(preprocess)\n#     print(text)\n#     seq = create_sequences(full_tokenizer, text, padding='post', maxlen=max_length, fit=False)\n\n#     preds = np.zeros(n_classes)\n#     for i, model in enumerate(model7):\n#         p = model.predict(seq)\n#         preds[i] = p\n     \n#     print(preds)\n#     predicted = np.argmax(preds)\n#     print(predicted)\n#     return ['negative', 'somewhat negative', 'neutral', 'somewhat positive', 'positive'][predicted]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(evaluate(\"I really liked this movie and wouldn't mind seeing it again!\"))\n# print(evaluate(\"I really hated this movie and never see it again!\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import CuDNNLSTM, GlobalAveragePooling1D, GlobalMaxPooling1D, Activation, Conv1D, MaxPooling1D\nfrom keras import regularizers\nfrom keras.callbacks import ModelCheckpoint\n\ndef build_model_v9(vocab_size, max_length,\n                   embedding_dims=100,\n                   embedding_matrix=None,\n                   n_classes=n_classes,\n                   rnn_units=128,\n                   dropout=0.5,\n                   loss='categorical_crossentropy',\n                   optimizer='adam',\n                   metrics=['acc']):\n    inputs = Input(shape=(max_length,))\n    \n    trainable=True\n    embedding_initializer='uniform'\n    if embedding_matrix is not None:\n        trainable=False\n        embedding_initializer=Constant(embedding_matrix)\n    \n    x = Embedding(vocab_size,\n                  embedding_dims,\n                  input_length=max_length,\n                  trainable=trainable,\n                  embeddings_initializer=embedding_initializer)(inputs)\n    x = SpatialDropout1D(dropout)(x)\n    \n    x = Bidirectional(CuDNNLSTM(rnn_units, return_sequences=True))(x)    \n    x = Conv1D(256, kernel_size=5)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = MaxPooling1D()(x)\n    x = Dropout(0.5)(x)\n    \n    x = Conv1D(128, kernel_size=3)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = MaxPooling1D()(x)\n    x = Dropout(0.5)(x)\n\n    x = Conv1D(64, kernel_size=2)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = MaxPooling1D()(x)\n    \n    x = Flatten()(x)\n    x = Dropout(0.5)(x)\n    \n    x = Dense(100, kernel_regularizer=regularizers.l2(0.001))(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(n_classes, activation = 'softmax')(x)\n\n    model = Model(inputs=[inputs], outputs=x)\n    model.compile(optimizer=optimizer,\n                  loss=loss,\n                  metrics=metrics)\n    \n    return model\n\nmodel9 = build_model_v9(len(tokenizer.word_index),\n                        seq_max_length,\n                        embedding_dims=100,\n                        rnn_units=128,\n                        dropout=0.7)\nmodel9.summary()\n\nfilepath=\"model9.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n\ntrain_model('model9', model9, \n            train_seq, y_train, test_seq, y_test, val_seq, y_val,\n            callbacks=[checkpoint],\n            class_weight=weights,\n            epochs=10,\n            batch_size=25,\n            save=False)","execution_count":26,"outputs":[{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_5 (InputLayer)         (None, 50)                0         \n_________________________________________________________________\nembedding_5 (Embedding)      (None, 50, 100)           1511400   \n_________________________________________________________________\nspatial_dropout1d_5 (Spatial (None, 50, 100)           0         \n_________________________________________________________________\nbidirectional_5 (Bidirection (None, 50, 256)           235520    \n_________________________________________________________________\nconv1d_13 (Conv1D)           (None, 46, 256)           327936    \n_________________________________________________________________\nbatch_normalization_15 (Batc (None, 46, 256)           1024      \n_________________________________________________________________\nactivation_15 (Activation)   (None, 46, 256)           0         \n_________________________________________________________________\nmax_pooling1d_13 (MaxPooling (None, 23, 256)           0         \n_________________________________________________________________\ndropout_16 (Dropout)         (None, 23, 256)           0         \n_________________________________________________________________\nconv1d_14 (Conv1D)           (None, 21, 128)           98432     \n_________________________________________________________________\nbatch_normalization_16 (Batc (None, 21, 128)           512       \n_________________________________________________________________\nactivation_16 (Activation)   (None, 21, 128)           0         \n_________________________________________________________________\nmax_pooling1d_14 (MaxPooling (None, 10, 128)           0         \n_________________________________________________________________\ndropout_17 (Dropout)         (None, 10, 128)           0         \n_________________________________________________________________\nconv1d_15 (Conv1D)           (None, 9, 64)             16448     \n_________________________________________________________________\nbatch_normalization_17 (Batc (None, 9, 64)             256       \n_________________________________________________________________\nactivation_17 (Activation)   (None, 9, 64)             0         \n_________________________________________________________________\nmax_pooling1d_15 (MaxPooling (None, 4, 64)             0         \n_________________________________________________________________\nflatten_5 (Flatten)          (None, 256)               0         \n_________________________________________________________________\ndropout_18 (Dropout)         (None, 256)               0         \n_________________________________________________________________\ndense_8 (Dense)              (None, 100)               25700     \n_________________________________________________________________\nbatch_normalization_18 (Batc (None, 100)               400       \n_________________________________________________________________\nactivation_18 (Activation)   (None, 100)               0         \n_________________________________________________________________\ndropout_19 (Dropout)         (None, 100)               0         \n_________________________________________________________________\ndense_9 (Dense)              (None, 5)                 505       \n=================================================================\nTotal params: 2,218,133\nTrainable params: 2,217,037\nNon-trainable params: 1,096\n_________________________________________________________________\nTrain on 124848 samples, validate on 15606 samples\nEpoch 1/10\n124848/124848 [==============================] - 110s 884us/step - loss: 1.2780 - acc: 0.5212 - val_loss: 1.0810 - val_acc: 0.5707\n\nEpoch 00001: val_acc improved from -inf to 0.57074, saving model to model9.h5\nEpoch 2/10\n124848/124848 [==============================] - 107s 860us/step - loss: 1.0052 - acc: 0.6050 - val_loss: 0.9060 - val_acc: 0.6393\n\nEpoch 00002: val_acc improved from 0.57074 to 0.63931, saving model to model9.h5\nEpoch 3/10\n124848/124848 [==============================] - 105s 845us/step - loss: 0.9061 - acc: 0.6427 - val_loss: 0.8634 - val_acc: 0.6507\n\nEpoch 00003: val_acc improved from 0.63931 to 0.65071, saving model to model9.h5\nEpoch 4/10\n124848/124848 [==============================] - 105s 839us/step - loss: 0.8649 - acc: 0.6574 - val_loss: 0.8464 - val_acc: 0.6537\n\nEpoch 00004: val_acc improved from 0.65071 to 0.65372, saving model to model9.h5\nEpoch 5/10\n124848/124848 [==============================] - 106s 846us/step - loss: 0.8383 - acc: 0.6698 - val_loss: 0.8502 - val_acc: 0.6552\n\nEpoch 00005: val_acc improved from 0.65372 to 0.65520, saving model to model9.h5\nEpoch 6/10\n124848/124848 [==============================] - 104s 830us/step - loss: 0.8173 - acc: 0.6796 - val_loss: 0.8547 - val_acc: 0.6523\n\nEpoch 00006: val_acc did not improve from 0.65520\nEpoch 7/10\n124848/124848 [==============================] - 104s 831us/step - loss: 0.8009 - acc: 0.6845 - val_loss: 0.8312 - val_acc: 0.6573\n\nEpoch 00007: val_acc improved from 0.65520 to 0.65731, saving model to model9.h5\nEpoch 8/10\n124848/124848 [==============================] - 105s 840us/step - loss: 0.7887 - acc: 0.6924 - val_loss: 0.8362 - val_acc: 0.6637\n\nEpoch 00008: val_acc improved from 0.65731 to 0.66372, saving model to model9.h5\nEpoch 9/10\n124848/124848 [==============================] - 103s 825us/step - loss: 0.7763 - acc: 0.6986 - val_loss: 0.8273 - val_acc: 0.6753\n\nEpoch 00009: val_acc improved from 0.66372 to 0.67525, saving model to model9.h5\nEpoch 10/10\n124848/124848 [==============================] - 102s 820us/step - loss: 0.7659 - acc: 0.7027 - val_loss: 0.8272 - val_acc: 0.6687\n\nEpoch 00010: val_acc did not improve from 0.67525\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"stream","text":"15606/15606 [==============================] - 3s 172us/step\nvalidation scores for model9\nloss score: 0.833715\nacc score: 0.666603\n","name":"stdout"},{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"<keras.callbacks.History at 0x7efd3a99c320>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_model('model9', model9, \n            train_seq, y_train, test_seq, y_test, val_seq, y_val,\n            initial_epoch=10,\n            callbacks=[checkpoint],\n            class_weight=weights,\n            epochs=15,\n            batch_size=25,\n            save=False)","execution_count":27,"outputs":[{"output_type":"stream","text":"Train on 124848 samples, validate on 15606 samples\nEpoch 11/15\n124848/124848 [==============================] - 106s 852us/step - loss: 0.7568 - acc: 0.7094 - val_loss: 0.8312 - val_acc: 0.6695\n\nEpoch 00011: val_acc did not improve from 0.67525\nEpoch 12/15\n 72675/124848 [================>.............] - ETA: 42s - loss: 0.7471 - acc: 0.7120","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-dbbb2a20c9b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             save=False)\n\u001b[0m","\u001b[0;32m<ipython-input-9-47b7238e12b8>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(name, model, x_train, y_train, x_test, y_test, x_val, y_val, batch_size, epochs, callbacks, class_weight, categorical, initial_epoch, save)\u001b[0m\n\u001b[1;32m     34\u001b[0m           \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m           initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# from sklearn.model_selection import GridSearchCV\n# from sklearn.linear_model import LogisticRegression\n# from scipy import sparse\n# from sklearn.preprocessing import StandardScaler\n# from nltk.corpus import stopwords\n\n# cv = CountVectorizer(min_df=5, max_df=0.8, ngram_range=(1,3), stop_words=stopwords.words('english'))\n# x = cv.fit_transform(train['CleanedPhrase'])\n# print('Shape of phrases: ', x.shape)\n\n# # add number of tokens\n# tokens = train['Tokens'].values.reshape((-1,1))\n# x = sparse.hstack((x, tokens))\n\n# print('Shape of new x: ', x.shape)\n\n# # standardize values\n# # scaler = StandardScaler(with_mean=False)\n# # x = scaler.fit_transform(x)\n\n# # add full sentence\n# # full = train['Full'].values.reshape((-1,1))\n# # x = sparse.hstack((x, full))\n# # print('Shape of training data: ', x.shape)\n\n# param_grid = {'C': [0.01, 0.1, 1, 10]}\n# grid = GridSearchCV(LogisticRegression(verbose=10), param_grid, cv=5, n_jobs=-1, verbose=100)\n# grid.fit(x, labels)\n\n# print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n# print(\"Best parameters: \", grid.best_params_)\n# print(\"Best estimator: \", grid.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import mglearn\n\n# feature_names = cv.get_feature_names()\n# feature_names = np.append(feature_names, np.array(['<length>', '<full sentence>']))\n\n# for cls, coefs in enumerate(grid.best_estimator_.coef_):\n#     mglearn.tools.visualize_coefficients(coefs, feature_names, n_top_features=25)\n#     plt.show()\n\n# # y_pred = classifier.predict(x_t)  \n\n# # print(confusion_matrix(y_test,y_pred))  \n# # print(classification_report(y_test,y_pred))  \n# # print(accuracy_score(y_test, y_pred)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Tokens'].hist()","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7f0199768a90>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Tokens'].max()","execution_count":28,"outputs":[{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"48"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(200)","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"     PhraseId  SentenceId  ...   Tokens   Full\n0    1         1           ...    35     True \n1    2         1           ...    14     False\n2    3         1           ...    2      False\n3    4         1           ...    1      False\n4    5         1           ...    1      False\n5    6         1           ...    12     False\n6    7         1           ...    1      False\n7    8         1           ...    11     False\n8    9         1           ...    1      False\n9    10        1           ...    10     False\n10   11        1           ...    3      False\n11   12        1           ...    1      False\n12   13        1           ...    2      False\n13   14        1           ...    1      False\n14   15        1           ...    1      False\n15   16        1           ...    7      False\n16   17        1           ...    1      False\n17   18        1           ...    6      False\n18   19        1           ...    1      False\n19   20        1           ...    5      False\n20   21        1           ...    1      False\n21   22        1           ...    4      False\n22   23        1           ...    1      False\n23   24        1           ...    3      False\n24   25        1           ...    1      False\n25   26        1           ...    2      False\n26   27        1           ...    1      False\n27   28        1           ...    21     False\n28   29        1           ...    21     False\n29   30        1           ...    2      False\n..   ..       ..           ...   ..        ...\n170  171       6           ...    4      False\n171  172       6           ...    3      False\n172  173       6           ...    2      False\n173  174       6           ...    1      False\n174  175       6           ...    1      False\n175  176       6           ...    1      False\n176  177       6           ...    12     False\n177  178       6           ...    12     False\n178  179       6           ...    5      False\n179  180       6           ...    1      False\n180  181       6           ...    4      False\n181  182       6           ...    1      False\n182  183       6           ...    3      False\n183  184       6           ...    2      False\n184  185       6           ...    1      False\n185  186       6           ...    1      False\n186  187       6           ...    7      False\n187  188       6           ...    1      False\n188  189       6           ...    6      False\n189  190       6           ...    3      False\n190  191       6           ...    2      False\n191  192       6           ...    1      False\n192  193       6           ...    1      False\n193  194       6           ...    3      False\n194  195       6           ...    1      False\n195  196       6           ...    2      False\n196  197       6           ...    1      False\n197  198       6           ...    1      False\n198  199       7           ...    8      True \n199  200       7           ...    1      False\n\n[200 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PhraseId</th>\n      <th>SentenceId</th>\n      <th>Phrase</th>\n      <th>CleanedPhrase</th>\n      <th>Tokens</th>\n      <th>Full</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .</td>\n      <td>a series of escapades demonstrating the adage that what is good for the goose is also good for the gander some of which occasionally amuses but none of which amounts to much of a story</td>\n      <td>35</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>A series of escapades demonstrating the adage that what is good for the goose</td>\n      <td>a series of escapades demonstrating the adage that what is good for the goose</td>\n      <td>14</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>A series</td>\n      <td>a series</td>\n      <td>2</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>A</td>\n      <td>a</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>series</td>\n      <td>series</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>1</td>\n      <td>of escapades demonstrating the adage that what is good for the goose</td>\n      <td>of escapades demonstrating the adage that what is good for the goose</td>\n      <td>12</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>1</td>\n      <td>of</td>\n      <td>of</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>1</td>\n      <td>escapades demonstrating the adage that what is good for the goose</td>\n      <td>escapades demonstrating the adage that what is good for the goose</td>\n      <td>11</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>1</td>\n      <td>escapades</td>\n      <td>escapades</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>1</td>\n      <td>demonstrating the adage that what is good for the goose</td>\n      <td>demonstrating the adage that what is good for the goose</td>\n      <td>10</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>11</td>\n      <td>1</td>\n      <td>demonstrating the adage</td>\n      <td>demonstrating the adage</td>\n      <td>3</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>12</td>\n      <td>1</td>\n      <td>demonstrating</td>\n      <td>demonstrating</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>13</td>\n      <td>1</td>\n      <td>the adage</td>\n      <td>the adage</td>\n      <td>2</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>14</td>\n      <td>1</td>\n      <td>the</td>\n      <td>the</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>15</td>\n      <td>1</td>\n      <td>adage</td>\n      <td>adage</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>16</td>\n      <td>1</td>\n      <td>that what is good for the goose</td>\n      <td>that what is good for the goose</td>\n      <td>7</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>17</td>\n      <td>1</td>\n      <td>that</td>\n      <td>that</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>18</td>\n      <td>1</td>\n      <td>what is good for the goose</td>\n      <td>what is good for the goose</td>\n      <td>6</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>19</td>\n      <td>1</td>\n      <td>what</td>\n      <td>what</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>20</td>\n      <td>1</td>\n      <td>is good for the goose</td>\n      <td>is good for the goose</td>\n      <td>5</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>21</td>\n      <td>1</td>\n      <td>is</td>\n      <td>is</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>22</td>\n      <td>1</td>\n      <td>good for the goose</td>\n      <td>good for the goose</td>\n      <td>4</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>23</td>\n      <td>1</td>\n      <td>good</td>\n      <td>good</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>24</td>\n      <td>1</td>\n      <td>for the goose</td>\n      <td>for the goose</td>\n      <td>3</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>25</td>\n      <td>1</td>\n      <td>for</td>\n      <td>for</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>26</td>\n      <td>1</td>\n      <td>the goose</td>\n      <td>the goose</td>\n      <td>2</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>27</td>\n      <td>1</td>\n      <td>goose</td>\n      <td>goose</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>28</td>\n      <td>1</td>\n      <td>is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .</td>\n      <td>is also good for the gander some of which occasionally amuses but none of which amounts to much of a story</td>\n      <td>21</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>29</td>\n      <td>1</td>\n      <td>is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story</td>\n      <td>is also good for the gander some of which occasionally amuses but none of which amounts to much of a story</td>\n      <td>21</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>30</td>\n      <td>1</td>\n      <td>is also</td>\n      <td>is also</td>\n      <td>2</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>170</th>\n      <td>171</td>\n      <td>6</td>\n      <td>of nearly epic proportions</td>\n      <td>of nearly epic proportions</td>\n      <td>4</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>171</th>\n      <td>172</td>\n      <td>6</td>\n      <td>nearly epic proportions</td>\n      <td>nearly epic proportions</td>\n      <td>3</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>172</th>\n      <td>173</td>\n      <td>6</td>\n      <td>nearly epic</td>\n      <td>nearly epic</td>\n      <td>2</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>173</th>\n      <td>174</td>\n      <td>6</td>\n      <td>nearly</td>\n      <td>nearly</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>174</th>\n      <td>175</td>\n      <td>6</td>\n      <td>epic</td>\n      <td>epic</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>175</th>\n      <td>176</td>\n      <td>6</td>\n      <td>proportions</td>\n      <td>proportions</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>176</th>\n      <td>177</td>\n      <td>6</td>\n      <td>rooted in a sincere performance by the title character undergoing midlife crisis .</td>\n      <td>rooted in a sincere performance by the title character undergoing midlife crisis</td>\n      <td>12</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>177</th>\n      <td>178</td>\n      <td>6</td>\n      <td>rooted in a sincere performance by the title character undergoing midlife crisis</td>\n      <td>rooted in a sincere performance by the title character undergoing midlife crisis</td>\n      <td>12</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>178</th>\n      <td>179</td>\n      <td>6</td>\n      <td>rooted in a sincere performance</td>\n      <td>rooted in a sincere performance</td>\n      <td>5</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>179</th>\n      <td>180</td>\n      <td>6</td>\n      <td>rooted</td>\n      <td>rooted</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>180</th>\n      <td>181</td>\n      <td>6</td>\n      <td>in a sincere performance</td>\n      <td>in a sincere performance</td>\n      <td>4</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>181</th>\n      <td>182</td>\n      <td>6</td>\n      <td>in</td>\n      <td>in</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>182</th>\n      <td>183</td>\n      <td>6</td>\n      <td>a sincere performance</td>\n      <td>a sincere performance</td>\n      <td>3</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>183</th>\n      <td>184</td>\n      <td>6</td>\n      <td>sincere performance</td>\n      <td>sincere performance</td>\n      <td>2</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>184</th>\n      <td>185</td>\n      <td>6</td>\n      <td>sincere</td>\n      <td>sincere</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>185</th>\n      <td>186</td>\n      <td>6</td>\n      <td>performance</td>\n      <td>performance</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>186</th>\n      <td>187</td>\n      <td>6</td>\n      <td>by the title character undergoing midlife crisis</td>\n      <td>by the title character undergoing midlife crisis</td>\n      <td>7</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>187</th>\n      <td>188</td>\n      <td>6</td>\n      <td>by</td>\n      <td>by</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>188</th>\n      <td>189</td>\n      <td>6</td>\n      <td>the title character undergoing midlife crisis</td>\n      <td>the title character undergoing midlife crisis</td>\n      <td>6</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>189</th>\n      <td>190</td>\n      <td>6</td>\n      <td>the title character</td>\n      <td>the title character</td>\n      <td>3</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>190</th>\n      <td>191</td>\n      <td>6</td>\n      <td>title character</td>\n      <td>title character</td>\n      <td>2</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>191</th>\n      <td>192</td>\n      <td>6</td>\n      <td>title</td>\n      <td>title</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>192</th>\n      <td>193</td>\n      <td>6</td>\n      <td>character</td>\n      <td>character</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>193</th>\n      <td>194</td>\n      <td>6</td>\n      <td>undergoing midlife crisis</td>\n      <td>undergoing midlife crisis</td>\n      <td>3</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>194</th>\n      <td>195</td>\n      <td>6</td>\n      <td>undergoing</td>\n      <td>undergoing</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>195</th>\n      <td>196</td>\n      <td>6</td>\n      <td>midlife crisis</td>\n      <td>midlife crisis</td>\n      <td>2</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>196</th>\n      <td>197</td>\n      <td>6</td>\n      <td>midlife</td>\n      <td>midlife</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>197</th>\n      <td>198</td>\n      <td>6</td>\n      <td>crisis</td>\n      <td>crisis</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>198</th>\n      <td>199</td>\n      <td>7</td>\n      <td>Narratively , Trouble Every Day is a plodding mess .</td>\n      <td>narratively trouble every day is a plodding mess</td>\n      <td>8</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>200</td>\n      <td>7</td>\n      <td>Narratively</td>\n      <td>narratively</td>\n      <td>1</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>200 rows × 6 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}